{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPSy3VPHL5Uq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from ResNet import Bottleneck, ResNet, ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFFakdIfNEEZ"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V2SCe0hDNeaV",
    "outputId": "02ea31cc-a3e8-4b9e-da02-1f2821ca4e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:13<00:00, 12382601.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVrCGU9jNkJb"
   },
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obf7QfWYOBFT"
   },
   "outputs": [],
   "source": [
    "net = ResNet50(10).to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sp3I3vApPK2w",
    "outputId": "60d8be96-76cf-4ca8-8451-9ef862d1ef19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [1, 100](epoch, minibatch):  0.0526393671054393\n",
      "Loss [1, 200](epoch, minibatch):  0.05295187274459749\n",
      "Loss [1, 300](epoch, minibatch):  0.05301217177882791\n",
      "Loss [2, 100](epoch, minibatch):  0.05588216789998114\n",
      "Loss [2, 200](epoch, minibatch):  0.05273983641527593\n",
      "Loss [2, 300](epoch, minibatch):  0.06406863600946963\n",
      "Loss [3, 100](epoch, minibatch):  0.04777993245050311\n",
      "Loss [3, 200](epoch, minibatch):  0.05010294391773641\n",
      "Loss [3, 300](epoch, minibatch):  0.048276498820632695\n",
      "Loss [4, 100](epoch, minibatch):  0.05068978223949671\n",
      "Loss [4, 200](epoch, minibatch):  0.0482586153713055\n",
      "Loss [4, 300](epoch, minibatch):  0.05181093121878803\n",
      "Loss [5, 100](epoch, minibatch):  0.044640032006427646\n",
      "Loss [5, 200](epoch, minibatch):  0.04661684276536107\n",
      "Loss [5, 300](epoch, minibatch):  0.047416857935022566\n",
      "Loss [6, 100](epoch, minibatch):  0.04445262691937387\n",
      "Loss [6, 200](epoch, minibatch):  0.04324642017483711\n",
      "Loss [6, 300](epoch, minibatch):  0.047380651608109475\n",
      "Loss [7, 100](epoch, minibatch):  0.04539223172701895\n",
      "Loss [7, 200](epoch, minibatch):  0.047040847060270605\n",
      "Loss [7, 300](epoch, minibatch):  0.04232197463978082\n",
      "Loss [8, 100](epoch, minibatch):  0.04728519815020263\n",
      "Loss [8, 200](epoch, minibatch):  0.04860822371672839\n",
      "Loss [8, 300](epoch, minibatch):  0.05020406272262335\n",
      "Loss [9, 100](epoch, minibatch):  0.044492414114065466\n",
      "Loss [9, 200](epoch, minibatch):  0.041711782598868014\n",
      "Loss [9, 300](epoch, minibatch):  0.044666750775650146\n",
      "Loss [10, 100](epoch, minibatch):  0.04452011676505208\n",
      "Loss [10, 200](epoch, minibatch):  0.04685863324906677\n",
      "Loss [10, 300](epoch, minibatch):  0.047288738060742616\n",
      "Loss [11, 100](epoch, minibatch):  0.04466859462670982\n",
      "Loss [11, 200](epoch, minibatch):  0.04876293934881687\n",
      "Loss [11, 300](epoch, minibatch):  0.04273700620047748\n",
      "Loss [12, 100](epoch, minibatch):  0.03937344547361135\n",
      "Loss [12, 200](epoch, minibatch):  0.0406868221051991\n",
      "Loss [12, 300](epoch, minibatch):  0.04192497312091291\n",
      "Loss [13, 100](epoch, minibatch):  0.04344244830310345\n",
      "Loss [13, 200](epoch, minibatch):  0.03650221494492143\n",
      "Loss [13, 300](epoch, minibatch):  0.046215824903920295\n",
      "Loss [14, 100](epoch, minibatch):  0.0424339847266674\n",
      "Loss [14, 200](epoch, minibatch):  0.04392548661679029\n",
      "Loss [14, 300](epoch, minibatch):  0.04056060338858515\n",
      "Loss [15, 100](epoch, minibatch):  0.04260369973257184\n",
      "Loss [15, 200](epoch, minibatch):  0.04289495999924839\n",
      "Loss [15, 300](epoch, minibatch):  0.04178761675488204\n",
      "Loss [16, 100](epoch, minibatch):  0.04416728910058737\n",
      "Loss [16, 200](epoch, minibatch):  0.04288411201909184\n",
      "Loss [16, 300](epoch, minibatch):  0.03772678068373352\n",
      "Loss [17, 100](epoch, minibatch):  0.04344683630857617\n",
      "Loss [17, 200](epoch, minibatch):  0.036312824566848576\n",
      "Loss [17, 300](epoch, minibatch):  0.04276587332831696\n",
      "Loss [18, 100](epoch, minibatch):  0.045755054200999436\n",
      "Loss [18, 200](epoch, minibatch):  0.042033363985829054\n",
      "Loss [18, 300](epoch, minibatch):  0.03710340029094368\n",
      "Loss [19, 100](epoch, minibatch):  0.03859791476977989\n",
      "Loss [19, 200](epoch, minibatch):  0.037921807919628917\n",
      "Loss [19, 300](epoch, minibatch):  0.03907887634821236\n",
      "Loss [20, 100](epoch, minibatch):  0.04039042253047228\n",
      "Loss [20, 200](epoch, minibatch):  0.04077422698028386\n",
      "Loss [20, 300](epoch, minibatch):  0.03998818809632212\n",
      "Loss [21, 100](epoch, minibatch):  0.03952447250019759\n",
      "Loss [21, 200](epoch, minibatch):  0.04179490545764566\n",
      "Loss [21, 300](epoch, minibatch):  0.04048370227217674\n",
      "Loss [22, 100](epoch, minibatch):  0.03737050416879356\n",
      "Loss [22, 200](epoch, minibatch):  0.04192988990340382\n",
      "Loss [22, 300](epoch, minibatch):  0.043327519111335276\n",
      "Loss [23, 100](epoch, minibatch):  0.04043291912879795\n",
      "Loss [23, 200](epoch, minibatch):  0.040530367637984455\n",
      "Loss [23, 300](epoch, minibatch):  0.04479229826945812\n",
      "Loss [24, 100](epoch, minibatch):  0.0411937641305849\n",
      "Loss [24, 200](epoch, minibatch):  0.03799001396633685\n",
      "Loss [24, 300](epoch, minibatch):  0.0425785927195102\n",
      "Loss [25, 100](epoch, minibatch):  0.03665296665625647\n",
      "Loss [25, 200](epoch, minibatch):  0.03745066119357943\n",
      "Loss [25, 300](epoch, minibatch):  0.0408933202479966\n",
      "Loss [26, 100](epoch, minibatch):  0.03586402212269604\n",
      "Loss [26, 200](epoch, minibatch):  0.032442461680620906\n",
      "Loss [26, 300](epoch, minibatch):  0.040401068446226415\n",
      "Loss [27, 100](epoch, minibatch):  0.041980696669779716\n",
      "Loss [27, 200](epoch, minibatch):  0.036791897364892065\n",
      "Loss [27, 300](epoch, minibatch):  0.04174530481453985\n",
      "Loss [28, 100](epoch, minibatch):  0.039720104716252536\n",
      "Loss [28, 200](epoch, minibatch):  0.0377838143799454\n",
      "Loss [28, 300](epoch, minibatch):  0.04041058428119868\n",
      "Loss [29, 100](epoch, minibatch):  0.03862846672069281\n",
      "Loss [29, 200](epoch, minibatch):  0.035999270398169754\n",
      "Loss [29, 300](epoch, minibatch):  0.03577042349148542\n",
      "Loss [30, 100](epoch, minibatch):  0.035965807407628746\n",
      "Loss [30, 200](epoch, minibatch):  0.036147769175004216\n",
      "Loss [30, 300](epoch, minibatch):  0.040367861655540765\n",
      "Loss [31, 100](epoch, minibatch):  0.036643050294369456\n",
      "Loss [31, 200](epoch, minibatch):  0.03187168746255338\n",
      "Loss [31, 300](epoch, minibatch):  0.03511923190206289\n",
      "Loss [32, 100](epoch, minibatch):  0.03639350445009768\n",
      "Loss [32, 200](epoch, minibatch):  0.035369949203450234\n",
      "Loss [32, 300](epoch, minibatch):  0.03732635777443647\n",
      "Loss [33, 100](epoch, minibatch):  0.03949114241404459\n",
      "Loss [33, 200](epoch, minibatch):  0.043251110021956266\n",
      "Loss [33, 300](epoch, minibatch):  0.04536542042624205\n",
      "Loss [34, 100](epoch, minibatch):  0.03664536160416901\n",
      "Loss [34, 200](epoch, minibatch):  0.04179688352160156\n",
      "Loss [34, 300](epoch, minibatch):  0.037379922466352584\n",
      "Loss [35, 100](epoch, minibatch):  0.03827850455418229\n",
      "Loss [35, 200](epoch, minibatch):  0.04017150537576526\n",
      "Loss [35, 300](epoch, minibatch):  0.03811742385383696\n",
      "Loss [36, 100](epoch, minibatch):  0.03705492485314608\n",
      "Loss [36, 200](epoch, minibatch):  0.03674289671704173\n",
      "Loss [36, 300](epoch, minibatch):  0.037516655884683135\n",
      "Loss [37, 100](epoch, minibatch):  0.03141373383346945\n",
      "Loss [37, 200](epoch, minibatch):  0.03813026955118403\n",
      "Loss [37, 300](epoch, minibatch):  0.03577222341671586\n",
      "Loss [38, 100](epoch, minibatch):  0.035345001267269256\n",
      "Loss [38, 200](epoch, minibatch):  0.029884103150106967\n",
      "Loss [38, 300](epoch, minibatch):  0.028106930709909647\n",
      "Loss [39, 100](epoch, minibatch):  0.021273591292556376\n",
      "Loss [39, 200](epoch, minibatch):  0.0254228249238804\n",
      "Loss [39, 300](epoch, minibatch):  0.026851014813873916\n",
      "Loss [40, 100](epoch, minibatch):  0.023585877981968224\n",
      "Loss [40, 200](epoch, minibatch):  0.024371878292877226\n",
      "Loss [40, 300](epoch, minibatch):  0.022126910821534694\n",
      "Loss [41, 100](epoch, minibatch):  0.022772176580037922\n",
      "Loss [41, 200](epoch, minibatch):  0.021037440192885698\n",
      "Loss [41, 300](epoch, minibatch):  0.02015725165139884\n",
      "Loss [42, 100](epoch, minibatch):  0.021584428844507782\n",
      "Loss [42, 200](epoch, minibatch):  0.019447068746667354\n",
      "Loss [42, 300](epoch, minibatch):  0.01931624593446031\n",
      "Loss [43, 100](epoch, minibatch):  0.019443261616397648\n",
      "Loss [43, 200](epoch, minibatch):  0.020646838920656593\n",
      "Loss [43, 300](epoch, minibatch):  0.021494473423808814\n",
      "Loss [44, 100](epoch, minibatch):  0.019293060817290097\n",
      "Loss [44, 200](epoch, minibatch):  0.020822077854536474\n",
      "Loss [44, 300](epoch, minibatch):  0.020760861665476114\n",
      "Loss [45, 100](epoch, minibatch):  0.017681493351701646\n",
      "Loss [45, 200](epoch, minibatch):  0.0188043853151612\n",
      "Loss [45, 300](epoch, minibatch):  0.0186174858326558\n",
      "Loss [46, 100](epoch, minibatch):  0.015283060236251913\n",
      "Loss [46, 200](epoch, minibatch):  0.019066826107446103\n",
      "Loss [46, 300](epoch, minibatch):  0.018670771388569846\n",
      "Loss [47, 100](epoch, minibatch):  0.018630194142460824\n",
      "Loss [47, 200](epoch, minibatch):  0.01700235061580315\n",
      "Loss [47, 300](epoch, minibatch):  0.019434490706771612\n",
      "Loss [48, 100](epoch, minibatch):  0.014463885361328721\n",
      "Loss [48, 200](epoch, minibatch):  0.018106560674495994\n",
      "Loss [48, 300](epoch, minibatch):  0.01705881000496447\n",
      "Loss [49, 100](epoch, minibatch):  0.018693586660083382\n",
      "Loss [49, 200](epoch, minibatch):  0.0169466931221541\n",
      "Loss [49, 300](epoch, minibatch):  0.02008319540414959\n",
      "Loss [50, 100](epoch, minibatch):  0.01726919055916369\n",
      "Loss [50, 200](epoch, minibatch):  0.015558878264855594\n",
      "Loss [50, 300](epoch, minibatch):  0.016345566399395467\n",
      "Loss [51, 100](epoch, minibatch):  0.01848443614784628\n",
      "Loss [51, 200](epoch, minibatch):  0.017969506637891753\n",
      "Loss [51, 300](epoch, minibatch):  0.017528565844986587\n",
      "Loss [52, 100](epoch, minibatch):  0.015987628846196457\n",
      "Loss [52, 200](epoch, minibatch):  0.017692475941730665\n",
      "Loss [52, 300](epoch, minibatch):  0.0178588101034984\n",
      "Loss [53, 100](epoch, minibatch):  0.016100902449106797\n",
      "Loss [53, 200](epoch, minibatch):  0.017007965171942488\n",
      "Loss [53, 300](epoch, minibatch):  0.016389330634847283\n",
      "Loss [54, 100](epoch, minibatch):  0.013502126220846549\n",
      "Loss [54, 200](epoch, minibatch):  0.015227302042767406\n",
      "Loss [54, 300](epoch, minibatch):  0.016165285034803675\n",
      "Loss [55, 100](epoch, minibatch):  0.015445727173937485\n",
      "Loss [55, 200](epoch, minibatch):  0.015507520929095336\n",
      "Loss [55, 300](epoch, minibatch):  0.018140382335986943\n",
      "Loss [56, 100](epoch, minibatch):  0.01601132907671854\n",
      "Loss [56, 200](epoch, minibatch):  0.015908067625714464\n",
      "Loss [56, 300](epoch, minibatch):  0.014758320078253746\n",
      "Loss [57, 100](epoch, minibatch):  0.017050488755339755\n",
      "Loss [57, 200](epoch, minibatch):  0.016856628088862635\n",
      "Loss [57, 300](epoch, minibatch):  0.014644595520803706\n",
      "Loss [58, 100](epoch, minibatch):  0.018147175557678565\n",
      "Loss [58, 200](epoch, minibatch):  0.015420141499489545\n",
      "Loss [58, 300](epoch, minibatch):  0.015195405376143754\n",
      "Loss [59, 100](epoch, minibatch):  0.01446633238112554\n",
      "Loss [59, 200](epoch, minibatch):  0.0123498177586589\n",
      "Loss [59, 300](epoch, minibatch):  0.014148920885054395\n",
      "Loss [60, 100](epoch, minibatch):  0.013828532480401919\n",
      "Loss [60, 200](epoch, minibatch):  0.012569601346040144\n",
      "Loss [60, 300](epoch, minibatch):  0.014517980923410506\n",
      "Loss [61, 100](epoch, minibatch):  0.012811494680354371\n",
      "Loss [61, 200](epoch, minibatch):  0.014001085825730114\n",
      "Loss [61, 300](epoch, minibatch):  0.012259507827693597\n",
      "Loss [62, 100](epoch, minibatch):  0.013763409317471087\n",
      "Loss [62, 200](epoch, minibatch):  0.014573077090317384\n",
      "Loss [62, 300](epoch, minibatch):  0.012319960123859346\n",
      "Loss [63, 100](epoch, minibatch):  0.014319361133966596\n",
      "Loss [63, 200](epoch, minibatch):  0.012180796189350076\n",
      "Loss [63, 300](epoch, minibatch):  0.014471035191090777\n",
      "Loss [64, 100](epoch, minibatch):  0.013327832369832322\n",
      "Loss [64, 200](epoch, minibatch):  0.01229087266488932\n",
      "Loss [64, 300](epoch, minibatch):  0.014426510717021302\n",
      "Loss [65, 100](epoch, minibatch):  0.0143933249800466\n",
      "Loss [65, 200](epoch, minibatch):  0.011426535962382332\n",
      "Loss [65, 300](epoch, minibatch):  0.015370116584235802\n",
      "Loss [66, 100](epoch, minibatch):  0.014244387203361839\n",
      "Loss [66, 200](epoch, minibatch):  0.012813591351732611\n",
      "Loss [66, 300](epoch, minibatch):  0.013046151592861861\n",
      "Loss [67, 100](epoch, minibatch):  0.01318216742540244\n",
      "Loss [67, 200](epoch, minibatch):  0.012980687166564166\n",
      "Loss [67, 300](epoch, minibatch):  0.012037846235907637\n",
      "Loss [68, 100](epoch, minibatch):  0.013027214620960877\n",
      "Loss [68, 200](epoch, minibatch):  0.013900275431806221\n",
      "Loss [68, 300](epoch, minibatch):  0.013629609678173438\n",
      "Loss [69, 100](epoch, minibatch):  0.013967026022728533\n",
      "Loss [69, 200](epoch, minibatch):  0.012369053438305855\n",
      "Loss [69, 300](epoch, minibatch):  0.014814386937650852\n",
      "Loss [70, 100](epoch, minibatch):  0.0107172735047061\n",
      "Loss [70, 200](epoch, minibatch):  0.013848206211114301\n",
      "Loss [70, 300](epoch, minibatch):  0.012641368788317777\n",
      "Loss [71, 100](epoch, minibatch):  0.01300693472614512\n",
      "Loss [71, 200](epoch, minibatch):  0.01298151291674003\n",
      "Loss [71, 300](epoch, minibatch):  0.015105189898167736\n",
      "Loss [72, 100](epoch, minibatch):  0.014811466578394175\n",
      "Loss [72, 200](epoch, minibatch):  0.011170448316261173\n",
      "Loss [72, 300](epoch, minibatch):  0.013182715168222786\n",
      "Loss [73, 100](epoch, minibatch):  0.01223634285503067\n",
      "Loss [73, 200](epoch, minibatch):  0.012760349125601351\n",
      "Loss [73, 300](epoch, minibatch):  0.013580595054663717\n",
      "Loss [74, 100](epoch, minibatch):  0.0121714548219461\n",
      "Loss [74, 200](epoch, minibatch):  0.010977434011292643\n",
      "Loss [74, 300](epoch, minibatch):  0.009675641051726415\n",
      "Loss [75, 100](epoch, minibatch):  0.01104033426148817\n",
      "Loss [75, 200](epoch, minibatch):  0.011605431084171869\n",
      "Loss [75, 300](epoch, minibatch):  0.011674766562064178\n",
      "Loss [76, 100](epoch, minibatch):  0.01479364876227919\n",
      "Loss [76, 200](epoch, minibatch):  0.013490412067621946\n",
      "Loss [76, 300](epoch, minibatch):  0.012434184348676354\n",
      "Loss [77, 100](epoch, minibatch):  0.01383089106529951\n",
      "Loss [77, 200](epoch, minibatch):  0.010593246202915907\n",
      "Loss [77, 300](epoch, minibatch):  0.01137534558889456\n",
      "Loss [78, 100](epoch, minibatch):  0.011648144940263592\n",
      "Loss [78, 200](epoch, minibatch):  0.012314927937695756\n",
      "Loss [78, 300](epoch, minibatch):  0.012477297798031941\n",
      "Loss [79, 100](epoch, minibatch):  0.013757644922588953\n",
      "Loss [79, 200](epoch, minibatch):  0.014076770899118856\n",
      "Loss [79, 300](epoch, minibatch):  0.01409291262505576\n",
      "Loss [80, 100](epoch, minibatch):  0.013766239834949375\n",
      "Loss [80, 200](epoch, minibatch):  0.012032560386578552\n",
      "Loss [80, 300](epoch, minibatch):  0.013107845592312515\n",
      "Loss [81, 100](epoch, minibatch):  0.010502666333923116\n",
      "Loss [81, 200](epoch, minibatch):  0.011884649737621657\n",
      "Loss [81, 300](epoch, minibatch):  0.010599789473344572\n",
      "Loss [82, 100](epoch, minibatch):  0.013891037735156715\n",
      "Loss [82, 200](epoch, minibatch):  0.011802421645843424\n",
      "Loss [82, 300](epoch, minibatch):  0.012339946454158052\n",
      "Loss [83, 100](epoch, minibatch):  0.011361622608383186\n",
      "Loss [83, 200](epoch, minibatch):  0.013197899137157947\n",
      "Loss [83, 300](epoch, minibatch):  0.010842944777105004\n",
      "Loss [84, 100](epoch, minibatch):  0.01573004090692848\n",
      "Loss [84, 200](epoch, minibatch):  0.0119206069316715\n",
      "Loss [84, 300](epoch, minibatch):  0.010829138872213661\n",
      "Loss [85, 100](epoch, minibatch):  0.01389050034689717\n",
      "Loss [85, 200](epoch, minibatch):  0.012267793822102248\n",
      "Loss [85, 300](epoch, minibatch):  0.013464167076162993\n",
      "Loss [86, 100](epoch, minibatch):  0.011238373541273177\n",
      "Loss [86, 200](epoch, minibatch):  0.011472576101659797\n",
      "Loss [86, 300](epoch, minibatch):  0.010468251419952139\n",
      "Loss [87, 100](epoch, minibatch):  0.011873848362592981\n",
      "Loss [87, 200](epoch, minibatch):  0.012740491571603342\n",
      "Loss [87, 300](epoch, minibatch):  0.013194658010615968\n",
      "Loss [88, 100](epoch, minibatch):  0.013073708710726351\n",
      "Loss [88, 200](epoch, minibatch):  0.012695907265879214\n",
      "Loss [88, 300](epoch, minibatch):  0.0118399870430585\n",
      "Loss [89, 100](epoch, minibatch):  0.0140984182379907\n",
      "Loss [89, 200](epoch, minibatch):  0.010606723858509214\n",
      "Loss [89, 300](epoch, minibatch):  0.012251125204493292\n",
      "Loss [90, 100](epoch, minibatch):  0.013153356558759696\n",
      "Loss [90, 200](epoch, minibatch):  0.013158954066457227\n",
      "Loss [90, 300](epoch, minibatch):  0.012052097112173215\n",
      "Loss [91, 100](epoch, minibatch):  0.011991634678561241\n",
      "Loss [91, 200](epoch, minibatch):  0.01126012791879475\n",
      "Loss [91, 300](epoch, minibatch):  0.013374659614055418\n",
      "Loss [92, 100](epoch, minibatch):  0.013621764057897962\n",
      "Loss [92, 200](epoch, minibatch):  0.013530195086495951\n",
      "Loss [92, 300](epoch, minibatch):  0.011934500950737857\n",
      "Loss [93, 100](epoch, minibatch):  0.01058401418151334\n",
      "Loss [93, 200](epoch, minibatch):  0.011486762362765149\n",
      "Loss [93, 300](epoch, minibatch):  0.012644988450338132\n",
      "Loss [94, 100](epoch, minibatch):  0.012039742090273649\n",
      "Loss [94, 200](epoch, minibatch):  0.01151225172332488\n",
      "Loss [94, 300](epoch, minibatch):  0.011592556366813368\n",
      "Loss [95, 100](epoch, minibatch):  0.012202977496781387\n",
      "Loss [95, 200](epoch, minibatch):  0.012252028346410952\n",
      "Loss [95, 300](epoch, minibatch):  0.012342057510977611\n",
      "Loss [96, 100](epoch, minibatch):  0.013440354838967323\n",
      "Loss [96, 200](epoch, minibatch):  0.009513426420162431\n",
      "Loss [96, 300](epoch, minibatch):  0.012686874644132332\n",
      "Loss [97, 100](epoch, minibatch):  0.0119165743899066\n",
      "Loss [97, 200](epoch, minibatch):  0.013754724719328806\n",
      "Loss [97, 300](epoch, minibatch):  0.010816744724288584\n",
      "Loss [98, 100](epoch, minibatch):  0.010500372254755348\n",
      "Loss [98, 200](epoch, minibatch):  0.01178796601598151\n",
      "Loss [98, 300](epoch, minibatch):  0.014706188351847232\n",
      "Loss [99, 100](epoch, minibatch):  0.013205190917942674\n",
      "Loss [99, 200](epoch, minibatch):  0.01352798093284946\n",
      "Loss [99, 300](epoch, minibatch):  0.012702522763283923\n",
      "Loss [100, 100](epoch, minibatch):  0.012321327381068841\n",
      "Loss [100, 200](epoch, minibatch):  0.010604351321235298\n",
      "Loss [100, 300](epoch, minibatch):  0.010366733066621236\n",
      "Loss [101, 100](epoch, minibatch):  0.012701518620597199\n",
      "Loss [101, 200](epoch, minibatch):  0.010920143547700718\n",
      "Loss [101, 300](epoch, minibatch):  0.012191299467813223\n",
      "Loss [102, 100](epoch, minibatch):  0.010481976977316662\n",
      "Loss [102, 200](epoch, minibatch):  0.011305982713820413\n",
      "Loss [102, 300](epoch, minibatch):  0.012004538535838946\n",
      "Loss [103, 100](epoch, minibatch):  0.011121568656526505\n",
      "Loss [103, 200](epoch, minibatch):  0.013534979720134288\n",
      "Loss [103, 300](epoch, minibatch):  0.011535572495195084\n",
      "Loss [104, 100](epoch, minibatch):  0.011368368645198643\n",
      "Loss [104, 200](epoch, minibatch):  0.011344387832796201\n",
      "Loss [104, 300](epoch, minibatch):  0.013381060002720914\n",
      "Loss [105, 100](epoch, minibatch):  0.012557924482389354\n",
      "Loss [105, 200](epoch, minibatch):  0.01232517866184935\n",
      "Loss [105, 300](epoch, minibatch):  0.010552530080312863\n",
      "Loss [106, 100](epoch, minibatch):  0.011250872350647114\n",
      "Loss [106, 200](epoch, minibatch):  0.01218798941175919\n",
      "Loss [106, 300](epoch, minibatch):  0.012087529049022123\n",
      "Loss [107, 100](epoch, minibatch):  0.011057785720913671\n",
      "Loss [107, 200](epoch, minibatch):  0.012471747170202434\n",
      "Loss [107, 300](epoch, minibatch):  0.010357074996572919\n",
      "Loss [108, 100](epoch, minibatch):  0.01172458573943004\n",
      "Loss [108, 200](epoch, minibatch):  0.012516164857661351\n",
      "Loss [108, 300](epoch, minibatch):  0.013365444022929296\n",
      "Loss [109, 100](epoch, minibatch):  0.012847774249967187\n",
      "Loss [109, 200](epoch, minibatch):  0.013124106365721673\n",
      "Loss [109, 300](epoch, minibatch):  0.012035601289826446\n",
      "Loss [110, 100](epoch, minibatch):  0.013397304661921225\n",
      "Loss [110, 200](epoch, minibatch):  0.013541310260770843\n",
      "Loss [110, 300](epoch, minibatch):  0.009627482502837665\n",
      "Loss [111, 100](epoch, minibatch):  0.013258730675443076\n",
      "Loss [111, 200](epoch, minibatch):  0.011582071073935368\n",
      "Loss [111, 300](epoch, minibatch):  0.01080412954906933\n",
      "Loss [112, 100](epoch, minibatch):  0.012919408037560061\n",
      "Loss [112, 200](epoch, minibatch):  0.012356444394681603\n",
      "Loss [112, 300](epoch, minibatch):  0.011958157979534008\n",
      "Loss [113, 100](epoch, minibatch):  0.010532438824884593\n",
      "Loss [113, 200](epoch, minibatch):  0.011340560021344571\n",
      "Loss [113, 300](epoch, minibatch):  0.014318015411263332\n",
      "Loss [114, 100](epoch, minibatch):  0.012346882266574538\n",
      "Loss [114, 200](epoch, minibatch):  0.011283607934601605\n",
      "Loss [114, 300](epoch, minibatch):  0.011229231655597686\n",
      "Loss [115, 100](epoch, minibatch):  0.013016788397799246\n",
      "Loss [115, 200](epoch, minibatch):  0.011663464542943985\n",
      "Loss [115, 300](epoch, minibatch):  0.012299543981207535\n",
      "Loss [116, 100](epoch, minibatch):  0.014263105539721438\n",
      "Loss [116, 200](epoch, minibatch):  0.012148464929778129\n",
      "Loss [116, 300](epoch, minibatch):  0.013413416247349232\n",
      "Loss [117, 100](epoch, minibatch):  0.01127687660686206\n",
      "Loss [117, 200](epoch, minibatch):  0.009885697849676944\n",
      "Loss [117, 300](epoch, minibatch):  0.013994358086492867\n",
      "Loss [118, 100](epoch, minibatch):  0.013103564584162087\n",
      "Loss [118, 200](epoch, minibatch):  0.01180456707952544\n",
      "Loss [118, 300](epoch, minibatch):  0.013165038535371423\n",
      "Loss [119, 100](epoch, minibatch):  0.011896217752364465\n",
      "Loss [119, 200](epoch, minibatch):  0.011249971020733938\n",
      "Loss [119, 300](epoch, minibatch):  0.01199022876447998\n",
      "Loss [120, 100](epoch, minibatch):  0.013225949064362794\n",
      "Loss [120, 200](epoch, minibatch):  0.013288730856147594\n",
      "Loss [120, 300](epoch, minibatch):  0.011805844290065579\n",
      "Loss [121, 100](epoch, minibatch):  0.013824647724395617\n",
      "Loss [121, 200](epoch, minibatch):  0.012571494976291432\n",
      "Loss [121, 300](epoch, minibatch):  0.012635288162855432\n",
      "Loss [122, 100](epoch, minibatch):  0.012186436482006685\n",
      "Loss [122, 200](epoch, minibatch):  0.011100000771693885\n",
      "Loss [122, 300](epoch, minibatch):  0.012575329937390052\n",
      "Loss [123, 100](epoch, minibatch):  0.010869384467368945\n",
      "Loss [123, 200](epoch, minibatch):  0.012159267307724804\n",
      "Loss [123, 300](epoch, minibatch):  0.010069156497484072\n",
      "Loss [124, 100](epoch, minibatch):  0.01219519941485487\n",
      "Loss [124, 200](epoch, minibatch):  0.01077730325749144\n",
      "Loss [124, 300](epoch, minibatch):  0.011588656650856137\n",
      "Loss [125, 100](epoch, minibatch):  0.012933383935596794\n",
      "Loss [125, 200](epoch, minibatch):  0.011658526679384522\n",
      "Loss [125, 300](epoch, minibatch):  0.012639242773875594\n",
      "Loss [126, 100](epoch, minibatch):  0.012188852101098746\n",
      "Loss [126, 200](epoch, minibatch):  0.011846815512981265\n",
      "Loss [126, 300](epoch, minibatch):  0.014054938394692726\n",
      "Loss [127, 100](epoch, minibatch):  0.012043117887806147\n",
      "Loss [127, 200](epoch, minibatch):  0.012993895962717944\n",
      "Loss [127, 300](epoch, minibatch):  0.012293274033581839\n",
      "Loss [128, 100](epoch, minibatch):  0.012377024786546826\n",
      "Loss [128, 200](epoch, minibatch):  0.012783431492280214\n",
      "Loss [128, 300](epoch, minibatch):  0.010837976657203399\n",
      "Loss [129, 100](epoch, minibatch):  0.010656941370107233\n",
      "Loss [129, 200](epoch, minibatch):  0.010606530796503649\n",
      "Loss [129, 300](epoch, minibatch):  0.010515873510739767\n",
      "Loss [130, 100](epoch, minibatch):  0.012636948028230108\n",
      "Loss [130, 200](epoch, minibatch):  0.012362465610494837\n",
      "Loss [130, 300](epoch, minibatch):  0.014115906793158502\n",
      "Loss [131, 100](epoch, minibatch):  0.011443182029761374\n",
      "Loss [131, 200](epoch, minibatch):  0.014298430338967591\n",
      "Loss [131, 300](epoch, minibatch):  0.010561129256384447\n",
      "Loss [132, 100](epoch, minibatch):  0.009448829837492668\n",
      "Loss [132, 200](epoch, minibatch):  0.009879369848640636\n",
      "Loss [132, 300](epoch, minibatch):  0.010403586698812433\n",
      "Loss [133, 100](epoch, minibatch):  0.010844474063487724\n",
      "Loss [133, 200](epoch, minibatch):  0.014126719909254461\n",
      "Loss [133, 300](epoch, minibatch):  0.011290996201569214\n",
      "Loss [134, 100](epoch, minibatch):  0.012230596283916383\n",
      "Loss [134, 200](epoch, minibatch):  0.010880449381656944\n",
      "Loss [134, 300](epoch, minibatch):  0.010725688729435206\n",
      "Loss [135, 100](epoch, minibatch):  0.01383820537594147\n",
      "Loss [135, 200](epoch, minibatch):  0.013098626434220932\n",
      "Loss [135, 300](epoch, minibatch):  0.012472559458110481\n",
      "Loss [136, 100](epoch, minibatch):  0.01242297385528218\n",
      "Loss [136, 200](epoch, minibatch):  0.012476080057676881\n",
      "Loss [136, 300](epoch, minibatch):  0.012350783668807708\n",
      "Loss [137, 100](epoch, minibatch):  0.01073959462082712\n",
      "Loss [137, 200](epoch, minibatch):  0.011438995887292549\n",
      "Loss [137, 300](epoch, minibatch):  0.014715535590657964\n",
      "Loss [138, 100](epoch, minibatch):  0.01208265769877471\n",
      "Loss [138, 200](epoch, minibatch):  0.012697081156075\n",
      "Loss [138, 300](epoch, minibatch):  0.011989486932288856\n",
      "Loss [139, 100](epoch, minibatch):  0.011530210039345547\n",
      "Loss [139, 200](epoch, minibatch):  0.01100074196467176\n",
      "Loss [139, 300](epoch, minibatch):  0.01194868394988589\n",
      "Loss [140, 100](epoch, minibatch):  0.010807631314964965\n",
      "Loss [140, 200](epoch, minibatch):  0.011422805860638619\n",
      "Loss [140, 300](epoch, minibatch):  0.013104684008285404\n",
      "Loss [141, 100](epoch, minibatch):  0.01214209541794844\n",
      "Loss [141, 200](epoch, minibatch):  0.012485770655330271\n",
      "Loss [141, 300](epoch, minibatch):  0.011244110670522786\n",
      "Loss [142, 100](epoch, minibatch):  0.010956412914674729\n",
      "Loss [142, 200](epoch, minibatch):  0.011617825686116703\n",
      "Loss [142, 300](epoch, minibatch):  0.014448218267643824\n",
      "Loss [143, 100](epoch, minibatch):  0.011183083788491786\n",
      "Loss [143, 200](epoch, minibatch):  0.011505062629585155\n",
      "Loss [143, 300](epoch, minibatch):  0.012599791691172868\n",
      "Loss [144, 100](epoch, minibatch):  0.012031478831195272\n",
      "Loss [144, 200](epoch, minibatch):  0.011897690759506077\n",
      "Loss [144, 300](epoch, minibatch):  0.01121643028454855\n",
      "Loss [145, 100](epoch, minibatch):  0.011793890796834603\n",
      "Loss [145, 200](epoch, minibatch):  0.012973489353898913\n",
      "Loss [145, 300](epoch, minibatch):  0.010272893238579854\n",
      "Loss [146, 100](epoch, minibatch):  0.012255461381864735\n",
      "Loss [146, 200](epoch, minibatch):  0.01306037396134343\n",
      "Loss [146, 300](epoch, minibatch):  0.012176837481092662\n",
      "Loss [147, 100](epoch, minibatch):  0.01294123669504188\n",
      "Loss [147, 200](epoch, minibatch):  0.011943732232321053\n",
      "Loss [147, 300](epoch, minibatch):  0.014489934939192609\n",
      "Loss [148, 100](epoch, minibatch):  0.013044192140805535\n",
      "Loss [148, 200](epoch, minibatch):  0.011667061423650012\n",
      "Loss [148, 300](epoch, minibatch):  0.011040420979261398\n",
      "Loss [149, 100](epoch, minibatch):  0.0106694169936236\n",
      "Loss [149, 200](epoch, minibatch):  0.011206223418703302\n",
      "Loss [149, 300](epoch, minibatch):  0.011236681364825927\n",
      "Loss [150, 100](epoch, minibatch):  0.010216287845978514\n",
      "Loss [150, 200](epoch, minibatch):  0.014555641753249802\n",
      "Loss [150, 300](epoch, minibatch):  0.01204196855018381\n",
      "Loss [151, 100](epoch, minibatch):  0.011684740051277913\n",
      "Loss [151, 200](epoch, minibatch):  0.011962508599972353\n",
      "Loss [151, 300](epoch, minibatch):  0.011638650698587298\n",
      "Loss [152, 100](epoch, minibatch):  0.01155551836360246\n",
      "Loss [152, 200](epoch, minibatch):  0.011109283858095296\n",
      "Loss [152, 300](epoch, minibatch):  0.011148391396855004\n",
      "Loss [153, 100](epoch, minibatch):  0.01239770921820309\n",
      "Loss [153, 200](epoch, minibatch):  0.012298901261528954\n",
      "Loss [153, 300](epoch, minibatch):  0.01136658939998597\n",
      "Loss [154, 100](epoch, minibatch):  0.01300848317798227\n",
      "Loss [154, 200](epoch, minibatch):  0.01235351484036073\n",
      "Loss [154, 300](epoch, minibatch):  0.010501463429536671\n",
      "Loss [155, 100](epoch, minibatch):  0.012544218454277143\n",
      "Loss [155, 200](epoch, minibatch):  0.010888894142699428\n",
      "Loss [155, 300](epoch, minibatch):  0.01157298784237355\n",
      "Loss [156, 100](epoch, minibatch):  0.011323649195837789\n",
      "Loss [156, 200](epoch, minibatch):  0.012494666348211468\n",
      "Loss [156, 300](epoch, minibatch):  0.009850670259911567\n",
      "Loss [157, 100](epoch, minibatch):  0.012374716277699918\n",
      "Loss [157, 200](epoch, minibatch):  0.010044927126727998\n",
      "Loss [157, 300](epoch, minibatch):  0.013734670638805256\n",
      "Loss [158, 100](epoch, minibatch):  0.013647240696009248\n",
      "Loss [158, 200](epoch, minibatch):  0.013505640195216984\n",
      "Loss [158, 300](epoch, minibatch):  0.012064715360756963\n",
      "Loss [159, 100](epoch, minibatch):  0.011244871020317078\n",
      "Loss [159, 200](epoch, minibatch):  0.011773448215099052\n",
      "Loss [159, 300](epoch, minibatch):  0.012192339661414735\n",
      "Loss [160, 100](epoch, minibatch):  0.011528881314443424\n",
      "Loss [160, 200](epoch, minibatch):  0.01286942901497241\n",
      "Loss [160, 300](epoch, minibatch):  0.00923990945564583\n",
      "Loss [161, 100](epoch, minibatch):  0.010962181708891875\n",
      "Loss [161, 200](epoch, minibatch):  0.011745551042840816\n",
      "Loss [161, 300](epoch, minibatch):  0.01079980360227637\n",
      "Loss [162, 100](epoch, minibatch):  0.01204495467885863\n",
      "Loss [162, 200](epoch, minibatch):  0.010026242545573041\n",
      "Loss [162, 300](epoch, minibatch):  0.011123222688911482\n",
      "Loss [163, 100](epoch, minibatch):  0.01338111471501179\n",
      "Loss [163, 200](epoch, minibatch):  0.011911492258077487\n",
      "Loss [163, 300](epoch, minibatch):  0.011773483490105718\n",
      "Loss [164, 100](epoch, minibatch):  0.011674656891264021\n",
      "Loss [164, 200](epoch, minibatch):  0.01211880084418226\n",
      "Loss [164, 300](epoch, minibatch):  0.012080103233456612\n",
      "Loss [165, 100](epoch, minibatch):  0.012118089090799913\n",
      "Loss [165, 200](epoch, minibatch):  0.012614395518321544\n",
      "Loss [165, 300](epoch, minibatch):  0.012715893491404132\n",
      "Loss [166, 100](epoch, minibatch):  0.009758758830139413\n",
      "Loss [166, 200](epoch, minibatch):  0.012226915480569004\n",
      "Loss [166, 300](epoch, minibatch):  0.010368221888784319\n",
      "Loss [167, 100](epoch, minibatch):  0.012579021222773008\n",
      "Loss [167, 200](epoch, minibatch):  0.012120983588974923\n",
      "Loss [167, 300](epoch, minibatch):  0.011254520620568656\n",
      "Loss [168, 100](epoch, minibatch):  0.01225720253540203\n",
      "Loss [168, 200](epoch, minibatch):  0.013914267002837733\n",
      "Loss [168, 300](epoch, minibatch):  0.01190301331691444\n",
      "Loss [169, 100](epoch, minibatch):  0.010462601821636781\n",
      "Loss [169, 200](epoch, minibatch):  0.010152192893438041\n",
      "Loss [169, 300](epoch, minibatch):  0.013377027674578131\n",
      "Loss [170, 100](epoch, minibatch):  0.011860218700021505\n",
      "Loss [170, 200](epoch, minibatch):  0.012581981322728097\n",
      "Loss [170, 300](epoch, minibatch):  0.012092187032685615\n",
      "Loss [171, 100](epoch, minibatch):  0.012004932530107908\n",
      "Loss [171, 200](epoch, minibatch):  0.010269735453184693\n",
      "Loss [171, 300](epoch, minibatch):  0.013168737239902839\n",
      "Loss [172, 100](epoch, minibatch):  0.01095512453932315\n",
      "Loss [172, 200](epoch, minibatch):  0.013270766703644767\n",
      "Loss [172, 300](epoch, minibatch):  0.010800197679782287\n",
      "Loss [173, 100](epoch, minibatch):  0.009993790041189641\n",
      "Loss [173, 200](epoch, minibatch):  0.011702454171609134\n",
      "Loss [173, 300](epoch, minibatch):  0.011800233928370289\n",
      "Loss [174, 100](epoch, minibatch):  0.011386798742460087\n",
      "Loss [174, 200](epoch, minibatch):  0.010665456005954184\n",
      "Loss [174, 300](epoch, minibatch):  0.011846000357763842\n",
      "Loss [175, 100](epoch, minibatch):  0.011679269853048027\n",
      "Loss [175, 200](epoch, minibatch):  0.012744669894455002\n",
      "Loss [175, 300](epoch, minibatch):  0.012098547825589777\n",
      "Loss [176, 100](epoch, minibatch):  0.013359843307407573\n",
      "Loss [176, 200](epoch, minibatch):  0.012333507941802964\n",
      "Loss [176, 300](epoch, minibatch):  0.013381156153627671\n",
      "Loss [177, 100](epoch, minibatch):  0.012358834183542057\n",
      "Loss [177, 200](epoch, minibatch):  0.011173793446505442\n",
      "Loss [177, 300](epoch, minibatch):  0.011983797416905872\n",
      "Loss [178, 100](epoch, minibatch):  0.010022666138247587\n",
      "Loss [178, 200](epoch, minibatch):  0.012238966888398864\n",
      "Loss [178, 300](epoch, minibatch):  0.011127251043799333\n",
      "Loss [179, 100](epoch, minibatch):  0.012959126955247485\n",
      "Loss [179, 200](epoch, minibatch):  0.010125921260332689\n",
      "Loss [179, 300](epoch, minibatch):  0.01558634527027607\n",
      "Loss [180, 100](epoch, minibatch):  0.011857655544299633\n",
      "Loss [180, 200](epoch, minibatch):  0.011513850725605153\n",
      "Loss [180, 300](epoch, minibatch):  0.010864673404721543\n",
      "Loss [181, 100](epoch, minibatch):  0.014809592288220302\n",
      "Loss [181, 200](epoch, minibatch):  0.011544767075683922\n",
      "Loss [181, 300](epoch, minibatch):  0.011217296355171128\n",
      "Loss [182, 100](epoch, minibatch):  0.011676495884312316\n",
      "Loss [182, 200](epoch, minibatch):  0.010571163151180372\n",
      "Loss [182, 300](epoch, minibatch):  0.011030004640342667\n",
      "Loss [183, 100](epoch, minibatch):  0.011835427799378523\n",
      "Loss [183, 200](epoch, minibatch):  0.00970974173862487\n",
      "Loss [183, 300](epoch, minibatch):  0.010291451287339442\n",
      "Loss [184, 100](epoch, minibatch):  0.013262973928358407\n",
      "Loss [184, 200](epoch, minibatch):  0.012204302037134766\n",
      "Loss [184, 300](epoch, minibatch):  0.01259422930830624\n",
      "Loss [185, 100](epoch, minibatch):  0.015450272035086527\n",
      "Loss [185, 200](epoch, minibatch):  0.01102298910729587\n",
      "Loss [185, 300](epoch, minibatch):  0.0096812573983334\n",
      "Loss [186, 100](epoch, minibatch):  0.01328912223340012\n",
      "Loss [186, 200](epoch, minibatch):  0.013081281230552123\n",
      "Loss [186, 300](epoch, minibatch):  0.00987370779446792\n",
      "Loss [187, 100](epoch, minibatch):  0.011979722158284857\n",
      "Loss [187, 200](epoch, minibatch):  0.010993009486119263\n",
      "Loss [187, 300](epoch, minibatch):  0.013751104417024181\n",
      "Loss [188, 100](epoch, minibatch):  0.011736999425338582\n",
      "Loss [188, 200](epoch, minibatch):  0.013185227797948755\n",
      "Loss [188, 300](epoch, minibatch):  0.012510924189118668\n",
      "Loss [189, 100](epoch, minibatch):  0.011509504869463854\n",
      "Loss [189, 200](epoch, minibatch):  0.013304230897920205\n",
      "Loss [189, 300](epoch, minibatch):  0.012752939279889687\n",
      "Loss [190, 100](epoch, minibatch):  0.011532035191776232\n",
      "Loss [190, 200](epoch, minibatch):  0.011428081408375874\n",
      "Loss [190, 300](epoch, minibatch):  0.012212769983452745\n",
      "Loss [191, 100](epoch, minibatch):  0.011569976147729904\n",
      "Loss [191, 200](epoch, minibatch):  0.011291385030490346\n",
      "Loss [191, 300](epoch, minibatch):  0.01154099178937031\n",
      "Loss [192, 100](epoch, minibatch):  0.012232263699406758\n",
      "Loss [192, 200](epoch, minibatch):  0.012674654238508084\n",
      "Loss [192, 300](epoch, minibatch):  0.013682476615067572\n",
      "Loss [193, 100](epoch, minibatch):  0.01020355788175948\n",
      "Loss [193, 200](epoch, minibatch):  0.012583127857069484\n",
      "Loss [193, 300](epoch, minibatch):  0.012762601046124472\n",
      "Loss [194, 100](epoch, minibatch):  0.012369154391926714\n",
      "Loss [194, 200](epoch, minibatch):  0.01378434602112975\n",
      "Loss [194, 300](epoch, minibatch):  0.011234624130884186\n",
      "Loss [195, 100](epoch, minibatch):  0.012432808668818325\n",
      "Loss [195, 200](epoch, minibatch):  0.013309815385146067\n",
      "Loss [195, 300](epoch, minibatch):  0.011339690723689273\n",
      "Loss [196, 100](epoch, minibatch):  0.011515411579748615\n",
      "Loss [196, 200](epoch, minibatch):  0.01131753875873983\n",
      "Loss [196, 300](epoch, minibatch):  0.01231170963961631\n",
      "Loss [197, 100](epoch, minibatch):  0.01185600391763728\n",
      "Loss [197, 200](epoch, minibatch):  0.01095514816173818\n",
      "Loss [197, 300](epoch, minibatch):  0.0120618079660926\n",
      "Loss [198, 100](epoch, minibatch):  0.013283595285611228\n",
      "Loss [198, 200](epoch, minibatch):  0.012466165461228229\n",
      "Loss [198, 300](epoch, minibatch):  0.013120808701496572\n",
      "Loss [199, 100](epoch, minibatch):  0.01118628543044906\n",
      "Loss [199, 200](epoch, minibatch):  0.010677333379280753\n",
      "Loss [199, 300](epoch, minibatch):  0.012555632499279455\n",
      "Loss [200, 100](epoch, minibatch):  0.010480701759806834\n",
      "Loss [200, 200](epoch, minibatch):  0.013018208015710115\n",
      "Loss [200, 300](epoch, minibatch):  0.010803124140948058\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "            \n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eY3rPw7wPVEe",
    "outputId": "213b5a1c-d9f4-4659-f6dc-b6ff9d0d1a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time :  0.49633002281188965\n",
      "Accuracy on 10,000 test images:  86.85000000000001 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('time : ',time.time() - start)\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = deepcopy(net)\n",
    "\n",
    "# 모든 레이어에 Unstructured Pruning 적용\n",
    "def apply_pruning_to_all_layers(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "            prune.remove(module, 'weight')\n",
    "apply_pruning_to_all_layers(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time :  0.512721061706543\n",
      "Accuracy on 10,000 test images:  86.0 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = pruned_model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('time : ',time.time() - start)\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_2 = deepcopy(net)\n",
    "\n",
    "# 모든 레이어에 Unstructured Pruning (amount=0.3) 적용\n",
    "def apply_pruning_to_all_layers(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.3)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "apply_pruning_to_all_layers(pruned_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time :  0.46253538131713867\n",
      "Accuracy on 10,000 test images:  85.45 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = pruned_model_2(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('time : ',time.time() - start)\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_3 = deepcopy(net)\n",
    "\n",
    "# 모든 레이어에 Unstructured Pruning (amount=0.1) 적용\n",
    "def apply_pruning_to_all_layers(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.1)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "apply_pruning_to_all_layers(pruned_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time :  0.4572627544403076\n",
      "Accuracy on 10,000 test images:  86.77 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = pruned_model_3(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('time : ',time.time() - start)\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR10-ResNet50_85%.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
